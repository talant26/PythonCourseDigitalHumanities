{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python For The Digital Humanities, Part 2: Practical Project: H.P. Lovecrafts *At the Mountains of Madness*\n",
        "Manuel Huth, 2026\n",
        "\n",
        "This file is part of a Python online course consisting of a reader and several scripts. You can find the entire course on GitHub (https://github.com/talant26). The course aims to introduce scholars of the humanities with no prior knowledge to the basics of Python, demonstrating typical applications in the humanities, such as extracting and visualising information from texts.\n",
        "\n",
        "\n",
        "## H.P. Lovecrafts *At the Mountains of Madness*\n",
        "* Source: https://www.gutenberg.org/ebooks/70652\n",
        "* https://en.wikipedia.org/wiki/At_the_Mountains_of_Madness\n",
        "\n",
        "## Goal:\n",
        "* Read the file\n",
        "* Extract information\n",
        "* Analyze the extracted data\n",
        "* Visualize the data\n",
        "\n",
        "We will get to know **different types of modules** that allow us to **work with text and web data**."
      ],
      "metadata": {
        "id": "vMePm5UB9S9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommendations for working with Google Colab:\n",
        "* Tutorial: https://www.tutorialspoint.com/google_colab/index.htm\n",
        "* Change the editor so that line numbers are displayed. (Tools -> Settings -> Editor -> show line numbers)\n",
        "* Google Colab does not permanently store files. To keep your files, download them or save them to Google Drive (https://www.tutorialspoint.com/google_colab/google_colab_saving_work.htm).\n",
        "* If errors occur when executing individual scripts, please click the 'Run all' button, which can be found either at the top of the menu bar or in the 'Runtime' > 'Run all' tab. This will execute all scripts in the correct order. This helps to avoid errors that may occur if the session has been inactive for too long, for example, or if a script accesses variables that have not been set previously.\n",
        "* If this does not work, you can try to restart a session (Runtime $>$ Restart Session).\n",
        "* For the scripts to work, make sure you have uploaded the files 'Copyright.txt', 'Mainpart.txt' and 'DoubleNames.csv'.\n",
        "\n",
        "Important note: The text of the novel 'Mountains of Madness' comes from the Gutenberg Project ( https://www.gutenberg.org/ebooks/70652 ). The text has been split into two parts 'Copyright.txt' and 'Mainpart.txt'. **Please note the copyright!**\n",
        "\n"
      ],
      "metadata": {
        "id": "vOuNPipxAmy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interacting with files\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bTeOFqgj_qrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with files can be very useful. I recommend using the *with open function*. It is similar to a function and takes the following arguments:\n",
        "- First of all the filename in quotes (e.g. 'starwars.txt')\n",
        "- Then we define the type of access we want. This is a single letter that tells the interpreter what we want to do. That is, we declare whether we want to read a file ('r'), write a file ('w'), or append something to the end of the file ('a'). You can combine the letters with a '+': use 'r+' to read and write to a file, use 'a+' to append to and read from a file, use 'w+' to read and write (it will also create the file if it does not already exist).\n",
        "- Sometimes you need to specify the encoding (for example: encoding='utf-8'). This will be explained in more detail during the course.\n",
        "\n",
        "Lets us look at an example. First, let’s open a not yet existing file for writing:"
      ],
      "metadata": {
        "id": "qGkPRpbcCoS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want writing access to the existing file 'starwars.txt'\n",
        "# It is encoded with 'utf-8'\n",
        "# We store the accessed file in the variable 'outputfile'\n",
        "# Using this variable we can access and change the file\n",
        "\n",
        "with open('starwars.txt', 'w+', encoding='utf-8') as outputfile:\n",
        "\n",
        "  # Now we write 'A long time ago...' into the file\n",
        "  outputfile.write('A long time ago...\\n')\n",
        "  outputfile.write('\\n')\n",
        "  outputfile.write('The end.')"
      ],
      "metadata": {
        "id": "hNps33rnCwLd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to open the file we just created and read its contents. There are two ways to do this: the *read method* and the *readlines method*.\n",
        "With the *read method* we can store the entire contents of the file in a single string variable, with the *readlines method* we can store all the lines of the file in a list (i.e. a list where each entry corresponds to one line of the file)."
      ],
      "metadata": {
        "id": "CuJ70Sb2CzAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The read method"
      ],
      "metadata": {
        "id": "-ZO4ESaNC3yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The following script will not work if you did not run the script above that created the starwars.txt file."
      ],
      "metadata": {
        "id": "lPpny5gQCm5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want reading access to the existing file 'starwars.txt'\n",
        "# It is encoded with 'utf-8'\n",
        "# We store the accessed file in the variable 'inputfile'\n",
        "with open('starwars.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  # Now we store the filecontent as a string in the variable 'content'\n",
        "  content = inputfile.read()\n",
        "  print(content)"
      ],
      "metadata": {
        "id": "E6C_ZhXqC6DE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The readlines method\n",
        "Now we want to read one of the files we just uploaded. So let us read the Copyright ('Copyright.txt') first.\n",
        "\n",
        "Note: If you have uploaded the file to a different folder (e.g. in your google drive), open the file explorer, right click on the file and hit 'copy path'.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sPIbv1ZPC_CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want reading access to the existing file 'starwars.txt'\n",
        "# It is encoded with 'utf-8'\n",
        "\n",
        "# We store the accessed file in the variable 'inputfile'\n",
        "with open('Copyright.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  # Now we store the filecontent as a string in the variable 'content'\n",
        "  content = inputfile.readlines()\n",
        "  print(content)\n",
        "\n",
        "  for element in content:\n",
        "    print(element[:-1])"
      ],
      "metadata": {
        "id": "ewu2qaJrDAjN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using the *readlines method* you can just loop through the elements of the file:"
      ],
      "metadata": {
        "id": "3jgVU0jvHFyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want reading access to the existing file 'starwars.txt'\n",
        "# It is encoded with 'utf-8'\n",
        "# We store the accessed file in the variable 'inputfile'\n",
        "with open('Copyright.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  for line in inputfile:\n",
        "\n",
        "    print('Content of the line:' + line[:-1])"
      ],
      "metadata": {
        "id": "yYIRhroPHHHw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://www.w3schools.com/python/python_file_handling.asp\n",
        "- https://www.geeksforgeeks.org/file-handling-python/\n",
        "- https://automatetheboringstuff.com/2e/chapter9/"
      ],
      "metadata": {
        "id": "ee5e8r-LHRX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project exercise: Read the Textfile\n",
        "\n",
        "Read the file 'Mainpart.txt' with the **read method**, store the text in variable and display the content of the variable. It is encoded with 'utf-8'. Display the result."
      ],
      "metadata": {
        "id": "P5eHjyCQHdax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Space for your code"
      ],
      "metadata": {
        "id": "f8RPgEWkHt9c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "YfbAIL86eAlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "myC6Zz8seCmg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## String functions\n",
        "\n",
        "Strings are similar to lists. To access certain parts of a string or to check if a string is inside another string, see the chapter on *Lists*.\n",
        "\n",
        "\n",
        "But there are also a number of methods specific to strings. Here are some\n",
        "important ones:\n",
        "\n",
        "Command    | Explanation\n",
        "-----------|----------------------------------------------------\n",
        "endswith() | Checks if a string ends with a specific value (e.g. a character).\n",
        "find() | Finds a string inside another string and returns the indexnumber, where the string was found.\n",
        "isdigit() | Checks if the characters of a string are digits.\n",
        "islower() | Checks if the characters of a string are lowercase.\n",
        "isupper() | Checks if the characters of a string are uppercase.\n",
        "replace() | Replaces parts of a string\n",
        "rfind() | See the find method, but rfind returns the indexnumber, where the string was found.\n",
        "split() | Splits a string at a separator (like a comma or semicolon for example). The result is stored as a list.\n",
        "splitlines() | Splits a string into lines. The result is stored as a list.\n",
        "startswith() | Checks if a string starts with a specific value (e.g. a character).\n",
        "strip() | Removes white spaces at the beginning and end of a string. Instead of white spaces other characters can be removed as well.\n",
        "\n",
        "\n",
        "For information about the methods and additional methods, see 'Further References'."
      ],
      "metadata": {
        "id": "wwYbCkjrIQ-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting strings: The *split method*\n",
        "\n",
        "You can split a string into a list with the split() method:"
      ],
      "metadata": {
        "id": "MapFTJseRN61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exampleString = 'This is an examplestring. It has more than one sentence.'\n",
        "\n",
        "# By default a string is split at each white space\n",
        "wordlist = exampleString.split()\n",
        "\n",
        "print(wordlist)\n",
        "\n",
        "# If you want to split at another separator, you can write it in the brackets\n",
        "wordlist = exampleString.split('.')\n",
        "\n",
        "print(wordlist)"
      ],
      "metadata": {
        "id": "tRe89gvfWg0T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The strip method\n",
        "With the strip method you can eiter remove white spaces or any characters at the beginning and end of a word:"
      ],
      "metadata": {
        "id": "RgKAXp0EVT2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing white spaces\n",
        "exampleWord = '    word           '\n",
        "exampleWord = exampleWord.strip()\n",
        "\n",
        "print(exampleWord)\n",
        "\n",
        "# Removing other characters\n",
        "exampleString = '#####***another string*****####'\n",
        "exampleString = exampleString.strip('#*')\n",
        "print(exampleString)"
      ],
      "metadata": {
        "id": "Mnt57uXjYLiz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://www.w3schools.com/python/python_strings_methods.asp\n",
        "- https://automatetheboringstuff.com/2e/chapter6/"
      ],
      "metadata": {
        "id": "ceZk50VBIWrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Exercise: Getting statistics about words\n",
        "\n",
        "**Exercise1:**\n",
        "\n",
        "Open the file 'Mainpart.txt' and save its text in a variable (you can copy the code, you used above). Now create a list containing all the words of the text (use white spaces as separators) and display the list. Each word shall appear only once in the list.\n",
        "\n",
        "**Exercise2 (Optional):**\n",
        "\n",
        "Change your code, so that the following characters at the end of each word are removed, before the word is added to the list: periods, commas, semicolons, question marks, and exclamation marks.\n",
        "\n",
        "**Exercise3 (Optional):**\n",
        "\n",
        "Display the length of the list (there are a few ways to do this).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HjgfdCMpIYGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Space for your code"
      ],
      "metadata": {
        "id": "A-OEOCArUEmY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "M1NtYOMaem5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Now let us create a list of each word\n",
        "  words = text.split()\n",
        "\n",
        "  # We create an empty list. Here we want to store each word only one time\n",
        "  filteredlist = []\n",
        "\n",
        "  # Now we loop through each word of the list words\n",
        "  for word in words:\n",
        "\n",
        "    # if the word is not in filteredlist, we append it to filteredlist\n",
        "    if word not in filteredlist:\n",
        "\n",
        "      filteredlist.append(word)\n",
        "\n",
        "  for entry in filteredlist:\n",
        "\n",
        "    print(entry)"
      ],
      "metadata": {
        "id": "tFIscptyephI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to exercise 2:**\n"
      ],
      "metadata": {
        "id": "lJAy-5QEf2k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Now let us create a list of each word\n",
        "  words = text.split()\n",
        "\n",
        "  # We create an empty list. Here we want to store each word only one time\n",
        "  filteredlist = []\n",
        "\n",
        "  # Now we loop through each word of the list words\n",
        "  for word in words:\n",
        "\n",
        "    ##############ADDITIONAL CODE##################################\n",
        "    # Let us remove unnecessary characters at the end of each word\n",
        "    word = word.strip(':.,;?!\"')\n",
        "\n",
        "    # if the word is not in filteredlist, we append it to filteredlist\n",
        "    if word not in filteredlist:\n",
        "\n",
        "      filteredlist.append(word)\n",
        "\n",
        "  for entry in filteredlist:\n",
        "\n",
        "    print(entry)"
      ],
      "metadata": {
        "id": "_Bxun3kagGlA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lj3oIH8gvYz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to exercise 3:**"
      ],
      "metadata": {
        "id": "vZu5kGmWgZhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Now let us create a list of each word\n",
        "  words = text.split()\n",
        "\n",
        "  # We create an empty list. Here we want to store each word only one time\n",
        "  filteredlist = []\n",
        "\n",
        "  # Now we loop through each word of the list words\n",
        "  for word in words:\n",
        "\n",
        "    # Let us remove unnecassary characters at the end of the words\n",
        "    word = word.strip(':.,;?!\"')\n",
        "\n",
        "    # if the word is not in filteredlist, we append it to filteredlist\n",
        "    if word not in filteredlist:\n",
        "\n",
        "      filteredlist.append(word)\n",
        "\n",
        "  # We display the length of the 'filteredlist'\n",
        "  print(len(filteredlist))"
      ],
      "metadata": {
        "id": "0SgyduZ_gzpo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegEx (Regular Expressions)\n",
        "\n"
      ],
      "metadata": {
        "id": "uFWfEYyXdqYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With **regular expressions**, you can search for strings that have a certain **pattern, such as phone numbers, dates, certain types of names... They can be very helpful and are essential when working with text.\n",
        "\n",
        "Creating patterns and working with regular expressions can be a bit tricky. Here is a simplified but working version of how to find all strings in a text that match a certain pattern:"
      ],
      "metadata": {
        "id": "BGXgnfNNw8o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the module\n",
        "First, you need to import regular expressions using the *import statement*."
      ],
      "metadata": {
        "id": "W3uBrg_Jy2h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "ivxyKH1gy5EZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "string = 'Hello \\n Hello'\n",
        "print(string)\n",
        "\n",
        "rawstring = (r'Hello \\n Hello')\n",
        "print(rawstring)"
      ],
      "metadata": {
        "id": "NirVy2CNP_CH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling a pattern\n",
        "Then we want to create a pattern that searches for strings consisting of 4 digits to find all the years in a text.\n",
        "\n",
        "`examplePattern = re.compile(r'[0-9]{4}')`\n",
        "\n",
        "**It consists of:**\n",
        "* a variable where we store the pattern in (here: `examplePattern`)\n",
        "* the command to create the pattern (here: `re.compile`)\n",
        "* a raw-string containing the pattern (here: `r'....'`)\n",
        "* the pattern (here: `[0-9]{4}`)\n",
        "\n",
        "**Patterns can consist of two parts:**\n",
        "- the characters that the pattern should have (here: `[0-9]`)\n",
        "- the desired number of characters (here: `{4}`)\n",
        "\n",
        "Characterpattern  | Explanation\n",
        "------------------|------------------------------------\n",
        "[A-Z]             | all uppercase characters from A-Z\n",
        "[a-z]             | all lowercase characters from a-z\n",
        "[0-9]             | all digits from 0-9\n",
        "[A-Zabc12]        | all uppercase characters from A-Z and the characters a, b, 1 and 2\n",
        "\n",
        "Characternumber   | Explanation\n",
        "------------------|------------------------------------\n",
        "{1}               | Exactly 1 character\n",
        "{1,}              | One or more characters\n",
        "{1,4}             | 1-4 characters\n",
        "{,4}              | 4 or less characters"
      ],
      "metadata": {
        "id": "dg2BT3dryqfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding a pattern\n",
        "After we have created the pattern we can search with it in a text. Here we store the results in the 'resultlist' variable:\n",
        "\n",
        "`resultlist = examplePattern.findall(text)`\n",
        "\n"
      ],
      "metadata": {
        "id": "lh3oo7S83AqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Our text\n",
        "text = 'Philipp Melanchthon was born in the year 1497 and died in the year 1560.'\n",
        "\n",
        "# Our pattern\n",
        "examplePattern = re.compile(r'[0-9]{4}')\n",
        "\n",
        "# Our result\n",
        "results = examplePattern.findall(text)\n",
        "\n",
        "# Let us print it\n",
        "print(results)"
      ],
      "metadata": {
        "id": "obTTCXJsfC7T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://automatetheboringstuff.com/2e/chapter7/\n",
        "- https://www.w3schools.com/python/python_regex.asp\n",
        "- **https://www.geeksforgeeks.org/regular-expression-python-examples/**"
      ],
      "metadata": {
        "id": "AlkZQIDOx2qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Exercises: Finding names, dates and chapters in the text"
      ],
      "metadata": {
        "id": "2h2FEgbU37I2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1:**\n",
        "\n",
        "Find all years in the novell mountains of madness."
      ],
      "metadata": {
        "id": "Mh40lP3X39RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n",
        "\n"
      ],
      "metadata": {
        "id": "obQpLAvO4rKn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2:**\n",
        "\n",
        "Find all roman numbers in the novel Mountains of Madness. Roman numbers can contain the following digits IVXLCDM\n",
        "\n",
        "The last digit is usually followed by a dot."
      ],
      "metadata": {
        "id": "PQxwZEAH41uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code"
      ],
      "metadata": {
        "id": "Iv9XAg_E41O2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3:**\n",
        "\n",
        "Find all names in the text. For the sake of simplicity, we assume that a name begins with a capital letter and is followed by one or more lowercase letters."
      ],
      "metadata": {
        "id": "uhM46DAl4ui4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n"
      ],
      "metadata": {
        "id": "ynW8icVy5gfg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4:**\n",
        "\n",
        "Find all combinations of two capitalized terms in the text (i.e. according to the following scheme: capitalized word, space, second capitalized word). This way we hope to find all first and last name combinations."
      ],
      "metadata": {
        "id": "v6xesYTm5joZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code"
      ],
      "metadata": {
        "id": "U63Kh6Wa5jD4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solutions"
      ],
      "metadata": {
        "id": "sHnghkKrhnzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to exercise 1**"
      ],
      "metadata": {
        "id": "zOUiouGRhqAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n",
        "  yearPattern = re.compile(r'[0-9]{4}')\n",
        "\n",
        "\n",
        "  print(yearPattern.findall(text))"
      ],
      "metadata": {
        "id": "xYmBMmvzh1cl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to Exercise 2:**\n",
        "\n",
        "Find all roman numbers in the novel Mountains of Madness. Roman numbers can contain the following digits IVXLCDM\n",
        "\n",
        "The last digit is usually followed by a dot."
      ],
      "metadata": {
        "id": "OPvZAhc0jQG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n",
        "  romanNumberPattern = re.compile(r'[IVXLCDM]{1,}[.]{1}')\n",
        "\n",
        "  print(romanNumberPattern.findall(text))"
      ],
      "metadata": {
        "id": "6xQer2BojgIz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations: We now know the number of chapters"
      ],
      "metadata": {
        "id": "Lx3PkGrxjwSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to exercise 3:**\n",
        "\n",
        "Find all names in the text. For the sake of simplicity, we assume that a name begins with a capital letter and is followed by one or more lowercase letters."
      ],
      "metadata": {
        "id": "5xurlhqskWp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n",
        "  namePattern = re.compile(r'[A-Z]{1}[a-z]{1,}')\n",
        "\n",
        "  print(namePattern.findall(text))"
      ],
      "metadata": {
        "id": "0ReepGhSkuco"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution to exercise 4:**\n",
        "\n",
        "Find all combinations of two capitalized terms in the text (i.e. according to the following scheme: capitalized word, space, second capitalized word). This way we hope to find all first and last name combinations."
      ],
      "metadata": {
        "id": "RuMlFKMhk80x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # Space for your code\n",
        "  doublenamePattern = re.compile(r'[A-Z]{1}[a-z]{1,}[ ]{1}[A-Z]{1}[a-z]{1,}')\n",
        "\n",
        "  print(doublenamePattern.findall(text))"
      ],
      "metadata": {
        "id": "XMmVN0s2lBxb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural language processing (NLP) with the Natural Language Toolkit\n",
        "**NLTK** (Natural Language Toolkit): It is a tool for analyzing and working\n",
        "with language using computational linguistics. Its data is based on ”50 corpora\n",
        "and lexical resources”.\n",
        "\n",
        "This module can do a lot of useful things. Here are just some examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "hsyxxl5f62qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the novel into sentences\n",
        "With the sentence tokenizer you can analyse a text and split it up into sentences. The result is stored as a list.\n",
        "\n"
      ],
      "metadata": {
        "id": "82VuSGaa-3RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to import the module\n",
        "import nltk\n",
        "\n",
        "# You may need to download the following package\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# We open our file\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "\n",
        "  # we read the file and store its content in the variable 'text'\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # We create a list of sentences with the help of the 'sentence tokenizer'\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "  for sentence in sentences:\n",
        "\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "qklWltiy75qq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a wordlist without stop words\n",
        "Stop words are the most common words (such as 'is' or 'and'). If we want to know, which words are in a text, we normally want to ignore them."
      ],
      "metadata": {
        "id": "TTGPHMk-_ymo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import nltk\n",
        "\n",
        "# You may need to download the following package\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# First we need to download the stopwords and import them separately\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# we want reading access to our file\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # We create a list of all words inside the text. Instead of using the split method we use the word tokenizer of nltk\n",
        "  words = nltk.word_tokenize(text)\n",
        "\n",
        "  # We create a set (something like a list) of stopwords\n",
        "  stopwordset = set(stopwords.words(\"english\"))\n",
        "\n",
        "  # We create an empty list. Here all words will be stored, that are not in our stopwordset\n",
        "  filteredlist = []\n",
        "\n",
        "  # We loop through the list of words\n",
        "  for word in words:\n",
        "\n",
        "    # We check if the word is not in the stopwordset\n",
        "    if word not in stopwordset:\n",
        "\n",
        "       # We check if the word is not yet in out filteredlist\n",
        "      if word not in filteredlist:\n",
        "\n",
        "        # We append it to our filtered list\n",
        "        filteredlist.append(word)\n",
        "\n",
        "  # we loop through each item of the filteredlist and display it\n",
        "  for item in filteredlist:\n",
        "\n",
        "    print(item)\n",
        "\n",
        "  # We display the length of the list = the number of all words in the list\n",
        "  print(len(filteredlist))\n"
      ],
      "metadata": {
        "id": "UoFUueJ7MDsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizing Words\n",
        "Now our list still has the disadvantage that some of its words have the same root (e.g. 'went', 'goes' and 'go' should not be different words). We can find their root with the help of a lemmatizer.\n",
        "\n",
        "\n",
        "Let's try this with the word 'goes'. Let us try to find its root."
      ],
      "metadata": {
        "id": "Kb_d1ENKPVvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import nltk\n",
        "\n",
        "# First we need to download the stopwords and import them separately\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# We may need to download 'wordnet'\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# We create our lemmatizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "# We lemmatize our word\n",
        "lemmatizedword = lemmatizer.lemmatize('goes')\n",
        "\n",
        "# Now the infinitive of the word will be displayed\n",
        "print(lemmatizedword)"
      ],
      "metadata": {
        "id": "EZlOQSqNPYxK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizing Words in the novel \"At the mountains of madness\"\n",
        "What we just did with one word, we want to apply to our project.Earlier, we made a list of the words in the novel 'At the Mountains of Madness'. Now we want to add only lemmatized words to the list (see Creating a Word List Without Stop Words). To do this, we need to make some small changes to the script we created earlier ."
      ],
      "metadata": {
        "id": "qJrr6nN1Qb06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import nltk\n",
        "\n",
        "# First we need to download the stopwords and import them separately\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "############ Additional Code #######\n",
        "# We may need to download 'wordnet'\n",
        "nltk.download('wordnet')\n",
        "# We create our lemmatizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "####################################\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # We create a list of all words inside the text. Instead of using the split method we use the word tokenizer of nltk\n",
        "  words = nltk.word_tokenize(text)\n",
        "\n",
        "  # We create a set (something like a list) of stopwords\n",
        "  stopwordset = set(stopwords.words(\"english\"))\n",
        "\n",
        "  # We create an empty list. Here all words will be stored, that are not in our stopwordset\n",
        "  filteredlist = []\n",
        "\n",
        "  # We loop through the list of words\n",
        "  for word in words:\n",
        "\n",
        "    ########################## Additional Code ######################\n",
        "    # Let us trace the word back to its root\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    #################################################################\n",
        "\n",
        "    # We check if the word is not in the stopwordset\n",
        "    if word not in stopwordset:\n",
        "\n",
        "       # We check if the word is not yet in out filteredlist\n",
        "      if word not in filteredlist:\n",
        "\n",
        "        # We append it to our filtered list\n",
        "        filteredlist.append(word)\n",
        "\n",
        "  # we loop through each item of the filteredlist and display it\n",
        "  for item in filteredlist:\n",
        "\n",
        "    print(item)\n",
        "\n",
        "  # We display the length of the list = the number of all words in the list\n",
        "  print(len(filteredlist))"
      ],
      "metadata": {
        "id": "W2tW05ZoQrTr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS-Tagging\n",
        "Part of speech tagging: Identification of words as verbs, nouns, adjectives ...\n",
        "\n",
        "The output is a list, that contains pairs of analyzed words and their function in the sentence. The pairs are stored as tuples (something similar to a list). Example:\n",
        "\n",
        "[('Luke', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('your', 'PRP$'), ('father', 'NN')]\n",
        "\n",
        "So we have the word 'Luke' and its function is 'NNP' (=proper noun), then we have 'I' and its part of speech is 'PRP' (Personal Pronoun) and so on...\n",
        "\n",
        "See the following list of abbreviations: https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html"
      ],
      "metadata": {
        "id": "9MebYWVBUxT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# You may need to install the following addittion with the following command\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# We have a wordlist, that we want to analyze\n",
        "wordlist = [ 'Luke', 'I', 'am', 'your', 'father' ]\n",
        "\n",
        "# Now we do the positional tagging\n",
        "taggedlist = nltk.pos_tag(wordlist)\n",
        "\n",
        "# Now we display taggedlist\n",
        "print(taggedlist)\n",
        "\n",
        "# Now lets assume we only want to display words, that are tagged with VBP\n",
        "# (i.e. Verb, non-3rd person singular present).\n",
        "\n",
        "# We loop through the pairs of the taggedlist\n",
        "for item in taggedlist:\n",
        "\n",
        "  # We check if the second item equals 'VBP' (i.e. the part os speech)\n",
        "  if item[1] == 'VBP':\n",
        "    # We disply the first item (i.e. the word)\n",
        "    print(item[0])\n",
        "\n",
        "# Now the word: \"am\" shoud be displayed"
      ],
      "metadata": {
        "id": "0Z_8K5o3VxHg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Exercise: POS-Tagging to find names / proper nouns\n",
        "\n",
        "The following program creates a list of all words in the novel 'At the Mountains of Madness'. Then it analyzes these words with POS tagging. Use the empty list 'filteredlist' and store in it only words that have been tagged as proper nouns ('NNP')."
      ],
      "metadata": {
        "id": "oUHfe1AQWovR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import nltk\n",
        "\n",
        "# You may need to install the following addittion\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # We create a list of all words inside the text. Instead of using the split method we use the word tokenizer of nltk\n",
        "  words = nltk.word_tokenize(text)\n",
        "\n",
        "  # Now we create a list containing word-POS pairs\n",
        "  taggedlist = nltk.pos_tag(words)\n",
        "\n",
        "  filteredlist = []\n",
        "  # Space for your code\n",
        "\n"
      ],
      "metadata": {
        "id": "6sMSy-bmW19K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "Hve8tcmAzrE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import nltk\n",
        "\n",
        "# You may need to install the following addittion\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "with open('Mainpart.txt', 'r', encoding='utf-8') as inputfile:\n",
        "  text = inputfile.read()\n",
        "\n",
        "  # We create a list of all words inside the text. Instead of using the split method we use the word tokenizer of nltk\n",
        "  words = nltk.word_tokenize(text)\n",
        "\n",
        "  # Now we create a list containing word-function pairs\n",
        "  taggedlist = nltk.pos_tag(words)\n",
        "\n",
        "  filteredlist = []\n",
        "  #################### Additional Code ##################\n",
        "  # We create an empty list 'filterdlist', where only thhe\n",
        "  # proper nouns shall be stored\n",
        "\n",
        "  # now we loop through each word-function pair\n",
        "  for pair in taggedlist:\n",
        "\n",
        "    # we check if the POS equals 'NNP' (=proper noun)\n",
        "    if pair[1] == 'NNP':\n",
        "\n",
        "      # Now we check, of the name inside the pair is already in filteredlist\n",
        "      if pair[0] not in filteredlist:\n",
        "\n",
        "        # We append the name to the filteredlist\n",
        "        filteredlist.append(pair[0])\n",
        "\n",
        "\n",
        "# Now we print each entry of the filteredlist\n",
        "for item in filteredlist:\n",
        "\n",
        "  print(item)\n"
      ],
      "metadata": {
        "id": "jvURASrzzsy-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- Website and introduction: https://www.nltk.org/\n",
        "- **Tutorial**: https://realpython.com/nltk-nlp-python/"
      ],
      "metadata": {
        "id": "L6Ic8b8a-0B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV\n",
        "CSV (comma separated values) is a standard format when it comes to storing or exchanging (large) data. You can think of a CSV file as a very basic form of an Excel spreadsheet.\n",
        "\n",
        "Rules for creating csv-files:\n",
        "- The first line contains the column titles.\n",
        "- A new line marks a new row and a comma marks a new column (see the following example)\n",
        "\n",
        "**Excel**\n",
        "\n",
        "Name | Age\n",
        "-----|-----\n",
        "John   | 32\n",
        "Maria  | 32\n",
        "Stephen| 44\n",
        "Hank   |12\n",
        "\n",
        "**CSV**\n",
        "```\n",
        "Name,Age\n",
        "John,32\n",
        "Maria,32\n",
        "Stephen,44\n",
        "Hank,12\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W1x9cmcVdRGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to create a CSV file\n",
        "Now let us create a csv file from the example above."
      ],
      "metadata": {
        "id": "0ESMINSJ3mpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us open and create a csv file\n",
        "with open('Examplecsv.csv', 'w+', encoding='utf-8') as outputfile:\n",
        "\n",
        "  # We create the first line which contains the header 'Name' and 'Age'\n",
        "  outputfile.write('Name,Age\\n')\n",
        "\n",
        "  # now we write the data\n",
        "  outputfile.write('John,32\\n')\n",
        "  outputfile.write('Maria,32\\n')\n",
        "  outputfile.write('Stephen,44\\n')\n",
        "  outputfile.write('Hank,12')"
      ],
      "metadata": {
        "id": "zMjkYh9D4Sof"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access the newly created file using the file browser. It is also possible to create csv files using the csv module (see 'Further Reading')."
      ],
      "metadata": {
        "id": "zQBNNoSi4Dbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing a CSV-file\n",
        "Importing a *csv file* can easily done with the *csv module*. It allows you to transform the *csv data* into python values. See the following example where we try to extract information from the file 'DoubleNames.csv'. (I created this file from data I extracted from the novel 'At the Mountains of Madness').The file has two columns. In the first one we have a double name (DoubleName), in the second one we have the number of times this double name appears in the novel (=Occurences)."
      ],
      "metadata": {
        "id": "J6VdeC0q1iCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we have to import the csv module\n",
        "import csv\n",
        "\n",
        "# we open the file\n",
        "with open('DoubleNames.csv', 'r', encoding='utf-8') as csvfile:\n",
        "\n",
        "  # we read it with the DictReader method\n",
        "  data = csv.DictReader(csvfile)\n",
        "\n",
        "  # now we display the data\n",
        "  for dictPerson in data:\n",
        "\n",
        "    # Now we display the data\n",
        "    print(dictPerson)"
      ],
      "metadata": {
        "id": "R8BbdOjw1-1E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://www.geeksforgeeks.org/working-csv-files-python/\n",
        "- https://automatetheboringstuff.com/2e/chapter16/"
      ],
      "metadata": {
        "id": "RJDJd6J72UXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requests and Beautiful soup\n",
        "With the *request module*, you can check the status of a website (i.e., see if it is online) or download its page source.\n",
        "\n",
        "In the following example, we will download and display the page source of the website 'w3schools.com/python'."
      ],
      "metadata": {
        "id": "CenrlmtHisLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the module\n",
        "import requests\n",
        "\n",
        "page = requests.get('https://w3schools.com/python/')\n",
        "\n",
        "print(page.text)"
      ],
      "metadata": {
        "id": "aJIQxRW_189Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to navigate through the tags of a web page or through an *xml file* (which may be important if you are creating an edition), you should use the *Beautiful Soup* module. For more information about this module, see 'Further references'."
      ],
      "metadata": {
        "id": "sudhiqYn2X7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "**The requests module**\n",
        "- https://www.w3schools.com/python/module_requests.asp\n",
        "- https://www.geeksforgeeks.org/python-requests-tutorial/\n",
        "- https://automatetheboringstuff.com/2e/chapter12/\n",
        "\n",
        "**The beautiful soup module**\n",
        "- https://beautiful-soup-4.readthedocs.io/en/latest/\n",
        "- https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/\n",
        "- https://automatetheboringstuff.com/2e/chapter12/"
      ],
      "metadata": {
        "id": "Qhzkf5jy1hTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON\n",
        "JSON is another important format for exchanging data, along with CSV. You can think of it as a combination of Python lists and dictionaries.\n",
        "\n",
        "See the following page for an example:\n",
        "https://lobid.org/gnd/139907211.json\n",
        "\n",
        "Let us try to get the page source of this site with the requests module and convert it to python values with the json module using the *json.loads* method:\n",
        "\n"
      ],
      "metadata": {
        "id": "x-k-xMuuiiHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need the json and requests modules\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# we access the website and download\n",
        "page = requests.get('https://lobid.org/gnd/139907211.json')\n",
        "pageSource = page.text\n",
        "\n",
        "# If you want to see the text of the website (=the variable PageSource)\n",
        "# just remove the hashtag at the beginning of the following line:\n",
        "# print(pageSource)\n",
        "\n",
        "# We convert pageSource using json loads and display it\n",
        "pythondata = json.loads(pageSource)\n",
        "print(pythondata)"
      ],
      "metadata": {
        "id": "yXBaq-Y45gPy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also convert Python values into a json file. For instructions on how to do this, see 'Further References'."
      ],
      "metadata": {
        "id": "uv2v6JlF65Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://automatetheboringstuff.com/2e/chapter16/\n",
        "- https://www.w3schools.com/python/python_json.asp\n",
        "- https://www.geeksforgeeks.org/python-json/"
      ],
      "metadata": {
        "id": "3x2fEXwv7GHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas\n",
        "Pandas is a powerful tool for data analysis. It is built on top of *Numpy*, but is easier to understand and work with.\n",
        "\n",
        "It is not only good for analyzing and exploring data, but also for cleaning and manipulating it (for example, you can remove duplicates), and even looking for correlations. We will focus on the analyzing and exploring part.\n",
        "\n",
        "An important aspect of understanding Pandas is that it uses dataframes.\n",
        "\n",
        "A dataframe is similar to an Excel spreadsheet or a csv file. Each column is in that dataframe called *series*.\n",
        "\n",
        "One of the most common cases of using Pandas is when you want to analyze data from a csv file."
      ],
      "metadata": {
        "id": "RsTI6Y9hidCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying a dataframe"
      ],
      "metadata": {
        "id": "dWjMQbywHqqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we import pandas and rename it to pd (this way we do not have to type so much)\n",
        "import pandas as pd\n",
        "\n",
        "# we read the csv-file and create a dataframe\n",
        "df = pd.read_csv('DoubleNames.csv')\n",
        "\n",
        "# we display the dataframe\n",
        "df"
      ],
      "metadata": {
        "id": "jdkjBcooHvxW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The describe method\n",
        "With the describe method, you can get statistical information for each column (like maximum, minimum, mean, standard deviation)."
      ],
      "metadata": {
        "id": "3Fi1GJ86MR7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we import pandas and rename it to pd (this way we do not have to type so much)\n",
        "import pandas as pd\n",
        "\n",
        "# we read the csv-file and create a dataframe\n",
        "df = pd.read_csv('DoubleNames.csv')\n",
        "\n",
        "# we 'describe' the dataframe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "BKAH3K3dMUhS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://www.w3schools.com/python/pandas/default.asp\n",
        "- https://www.geeksforgeeks.org/pandas-tutorial/\n"
      ],
      "metadata": {
        "id": "z-yge1VTErX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matplotlib\n",
        "Using the dataframes we created in Pandas we can now visualize them with matplotlib. See the examples below. For tutorials and instructions, see 'Further References'"
      ],
      "metadata": {
        "id": "04QutBMUimc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter plot"
      ],
      "metadata": {
        "id": "0cMZShVaOmfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we import pandas and name it pd\n",
        "import pandas as pd\n",
        "# we import matplotlib and name it plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# we create a pandas dataframe\n",
        "df = pd.read_csv(\"DoubleNames.csv\")\n",
        "\n",
        "# We create a scatter plot, using the columns of the dataframe\n",
        "plt.scatter(df['DoubleName'], df['Occurences'])\n",
        "\n",
        "# We add a title\n",
        "plt.title(\"Scatter Plot\")\n",
        "\n",
        "# we name the x-axis and y-axis\n",
        "plt.xlabel('DoubleName')\n",
        "plt.ylabel('Occurences')\n",
        "\n",
        "# we display the scatter plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z2BgISWhNqas"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Line Chart"
      ],
      "metadata": {
        "id": "ghObYyZcOfDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we import pandas and name it pd\n",
        "import pandas as pd\n",
        "# we import matplotlib and name it plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# we create a pandas dataframe\n",
        "df = pd.read_csv(\"DoubleNames.csv\")\n",
        "\n",
        "# We create a line chart, using the columns of the dataframe\n",
        "plt.plot(df['DoubleName'], df['Occurences'])\n",
        "\n",
        "# We add a title\n",
        "plt.title(\"Line Chart\")\n",
        "\n",
        "# we name the x-axis and y-axis\n",
        "plt.xlabel('DoubleName')\n",
        "plt.ylabel('Occurences')\n",
        "\n",
        "# we display the line chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CL_vUudOA8G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar Chart"
      ],
      "metadata": {
        "id": "aEDFxrxHOwum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we import pandas and name it pd\n",
        "import pandas as pd\n",
        "# we import matplotlib and name it plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We create a dataframe with pandas\n",
        "data = pd.read_csv(\"DoubleNames.csv\")\n",
        "\n",
        "# we create a bar chart using the columns of the dataframe\n",
        "plt.bar(data['DoubleName'], data['Occurences'])\n",
        "\n",
        "# We add a title\n",
        "plt.title(\"Bar Chart\")\n",
        "\n",
        "# We name the X- and Y-Axis\n",
        "plt.xlabel('DoubleName')\n",
        "plt.ylabel('Occurences')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eWpoH4auPPHb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further References\n",
        "- https://www.geeksforgeeks.org/data-visualization-with-python/\n",
        "- https://www.geeksforgeeks.org/matplotlib-tutorial/"
      ],
      "metadata": {
        "id": "e891kOPGF1Dd"
      }
    }
  ]
}
